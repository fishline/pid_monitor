#!/usr/bin/perl
use strict;
use warnings;
use lib qw(..);
use JSON qw( );
use File::Basename;

if ($#ARGV + 1 != 1) {
    die "Usage: ./generate_scripts.pl <TEST_CASE_TAG>";
}

my $case_tag = $ARGV[0];
if (not ((-e $case_tag."-spark-conf.json") and (-e $case_tag."-scenario.json"))) {
    die "Please define JSON files for $case_tag, take example*.json as reference!";
}
my $case_spark_conf_fn = $case_tag."-spark-conf.json";
my $case_scenario_fn = $case_tag."-scenario.json";

########## Load JSON definition from above two files ############
my $spark_conf_text = do {
    open(my $json_fh, "<:encoding(UTF-8)", $case_spark_conf_fn) or die "Cannot open $case_spark_conf_fn for read!";
    local $/;
    <$json_fh>
};
my $json = JSON->new;
my $spark_conf = $json->decode($spark_conf_text);
# Sanity check
if (not (exists $spark_conf->{"MASTER"} and exists $spark_conf->{"SPARK_HOME"} and exists $spark_conf->{"HADOOP_HOME"})) {
    die "Please define MASTER/SPARK_HOME/HADOOP_HOME in $case_spark_conf_fn";
}
if ($spark_conf->{"SCHEDULER"} ne "YARN") {
    die "Does not support ".$spark_conf->{"SCHEDULER"}." currently, only YARN mode supported";
}

my $scenario_text = do {
    open(my $json_fh, "<:encoding(UTF-8)", $case_scenario_fn) or die "Cannot open $case_scenario_fn for read!";
    local $/;
    <$json_fh>
};
my $scenario = $json->decode($scenario_text);

########### Verify the environment as defined in the JSON files ############
# Check MASTE is current node
my $ping_result = `ping $spark_conf->{"MASTER"} -c 1`;
if ($? != 0) {
    die "Please make sure to run the script from ".$spark_conf->{"MASTER"};
}
`ping $spark_conf->{"MASTER"} -c 1 | head -n 1 | awk -F\\( '{print \$2}' | awk -F\\) '{print \$1}' | xargs -i sh -c "ifconfig | grep {}"`;
if ($? != 0) {
    die "Please make sure to run the script from ".$spark_conf->{"MASTER"};
}

# Get all nodes and check ssh/dstat etc
if (not (-e $spark_conf->{"HADOOP_HOME"}."/etc/hadoop/slaves")) {
    die "slaves file not found under ".$spark_conf->{"HADOOP_HOME"}."/etc/hadoop/ folder";
}
my $slaves_str = `grep -v # $spark_conf->{"HADOOP_HOME"}/etc/hadoop/slaves`;
my @nodes = split(/\n/, $slaves_str);
push (@nodes, $spark_conf->{"MASTER"});
if ($#nodes == 0) {
    die "Slave nodes not defined in ".$spark_conf->{"HADOOP_HOME"}."/etc/hadoop/slaves";
}
my $need_install_tools = 0;
my $ssh_problem = 0;
foreach my $node (@nodes) {
    `ssh $node date`;
    if ($? != 0) {
        $ssh_problem = 1;
        print "$node not reachable or passwordless not set\n";
    } else {
        `ssh $node mkdir /tmp/sparkLogs > /dev/null 2>&1`;
        `ssh $node which dstat`;
        if ($? != 0) {
            $need_install_tools = 1;
            print "Please install dstat on $node\n";
        }
    }
}
if ($ssh_problem != 0) {
    die "Please resolve ssh passwordless access first";
}
if ($need_install_tools != 0) {
    die "Install tools and try again";
}

# Upload lpcpu to the first slave node
if (not (-e "../../../lpcpu.tar.bz2")) {
    die "lpcpu.tar.bz2 not found in repository";
}
`scp ../../../lpcpu.tar.bz2 $nodes[0]:/root/`;
`ssh $nodes[0] "cd /root && tar xjf lpcpu.tar.bz2"`;

########### Generate the test scripts ############
my $date_str = `date +"%Y%m%d%H%M%S"`;
chomp($date_str);
my $script_dir = $case_tag."-".$date_str;
`mkdir $script_dir`;
open my $script_fh, "> $script_dir/run.sh" or die "Cannot open file ".$script_dir."/run.sh for write";

my $pmh = dirname(dirname(dirname(`pwd`)));
print $script_fh <<EOF;
#!/bin/bash
# This script is generated by generate_scripts.pl
export PMH=$pmh
export WORKLOAD_NAME=$script_dir
export DESCRIPTION="$script_dir"
export WORKLOAD_DIR="."      # The workload working directory
export MEAS_DELAY_SEC=1      # Delay between each measurement
export RUNDIR=\$(\${PMH}/setup-run.sh \$WORKLOAD_NAME)
EOF

close $script_fh;
`chmod +x $script_dir/run.sh`;
